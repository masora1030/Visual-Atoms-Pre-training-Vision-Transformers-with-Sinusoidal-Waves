<!DOCTYPE html>
<html>
<head>
	<title>Visual Atoms: Pre-training Vision Transformers with Sinusoidal Wave</title>
    <link rel="stylesheet" type="text/css" href="./pvg.css">
    <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
    <!--<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">-->
</head>

<body>
<script type="text/javascript" src="./header.js"></script>

<style>
a.myclass {
    color:#DE382D;
    text-decoration: underline
}
</style>

<style>
a.link {
    text-decoration: underline
}
</style>

<h1 align="center" style="font-size: 30pt;"><b>Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves</b></h1><br/>
<center>
    <font color="#c7254e" font size="4"><b>IEEE/CVF International Conference on Computer Vision and Pattern Recognition<br>(CVPR 2023)</b></font><br><br>
    <a href="https://masora1030.github.io/soraemonpockt/" class="">Sora Takashima</a><sup>1,2</sup> &emsp;
    <a href="https://sites.google.com/view/ryo-hayamizu" class="">Ryo Hayamizu</a><sup>1</sup> &emsp;
    <a href="https://mmai.tech/" class="">Nakamasa Inoue</a><sup>1,2</sup> &emsp;
    <a href="http://hirokatsukataoka.net/" class="">Hirokatsu Kataoka</a><sup>1</sup> &emsp; 
    <a href="https://www.rio.gsic.titech.ac.jp/en/index.html" class="">Rio Yokota</a><sup>1,2</sup> &emsp;
<br>
    1: AIST &emsp; 2: TITech<br><br>
    
    <section class="delta">
        <div class="container">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Takashima_Visual_Atoms_Pre-Training_Vision_Transformers_With_Sinusoidal_Waves_CVPR_2023_paper.pdf"><button class="btn btn-gray">Paper</button></a>
            <a href="https://github.com/masora1030/CVPR2023-FDSL-on-VisualAtom"><button class="btn btn-gray">Code</button></a>
            <a href="#dataset"><button class="btn btn-gray">Dataset</button></a>
            <a href="https://cvpr2023.thecvf.com/media/PosterPDFs/CVPR%202023/22854.png?t=1685632285.9741583"><button class="btn btn-gray">Poster</button></a>
            <a href="./CVPR2023_VisualAtom_FDSL_Supplementary_Material.pdf"><button class="btn btn-gray">Supp</button></a> <br>

            <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
            <a href="https://hirokatsukataoka16.github.io/Pretraining-without-Natural-Images/"><button class="btn btn-gray">Related Work 1 (IJCV22)<br>FDSL Proposal</button></a>
            <a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/"><button class="btn btn-gray">Related Work 2 (AAAI22)<br>ViT Pre-training</button></a>
            <a href="https://hirokatsukataoka16.github.io/Replacing-Labeled-Real-Image-Datasets/"><button class="btn btn-gray">Related Work 3 (CVPR22)<br>ExFractalDB, RCDB</button></a>
        </div>
    </section>
    <br><br>
    <iframe width="800" height="450" src="https://www.youtube.com/embed/2reoDrFf0OA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<br>
<h2>Abstract</h2>
<p>
Formula-driven supervised learning (FDSL) has been shown to be an effective method for pre-training vision transformers, where ExFractalDB-21k was shown to exceed the pre-training effect of ImageNet-21k. These studies also indicate that contours mattered more than textures when pre-training vision transformers. However, the lack of a systematic investigation as to why these contour-oriented synthetic datasets can achieve the same accuracy as real datasets leaves much room for skepticism. <font color="#c7254e" font size="5"><b>In the present work, we develop a novel methodology based on circular harmonics for systematically investigating the design space of contour-oriented synthetic datasets. This allows us to efficiently search the optimal range of FDSL parameters and maximize the variety of synthetic images in the dataset, which we found to be a critical factor. When the resulting new dataset VisualAtom-21k is used for pre-training ViT-Base, the top-1 accuracy reached 83.7% when fine-tuning on ImageNet-1k. This is only 0.5% difference from the top-1 accuracy (84.2%) achieved by the JFT-300M pre-training, even though the scale of images is 1/14.</b></font> Unlike JFT-300M which is a static dataset, the quality of synthetic datasets will continue to improve, and the current work is a testament to this possibility. FDSL is also free of the common issues associated with real images, e.g. privacy/copyright issues, labeling costs/errors, and ethical biases.
</p>

<br><br><br>
<center>
    <img src="./img/header_first.png" style="width: 40%;"/>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <img src="./img/header_4.png" style="width: 24%;"/>
</center>

<br><br><br>
<h2>Hypothes</h2>
To enhance the performance of FDSL, we test the following hypothes:<br><br><br>

<font size="4">Hypothes: The Variation of Object Contours is what matter in FDSL datasets</font>
<ul>
  <li>Related works about FDSL have shown that the pre-training ViT focuses on the object contours of images</li>
  <li>We proposed a new FDSL dataset â€“ VisualAtom with sinusoidal waves for improving the design variation of contours</li>
  <li>Brought variety to the contours, VisualAtom outperformed latest SoTA FDSL datasets (e.g., RCDB and ExFractalDB)</li>
</ul>
<br>
<center>
        <img src="./img/creation_visualatom.png" style="width: 80%;"/><br><br><br>
        <img src="./img/compare_with_other_fdsl.png" style="width: 50%;"/>
</center>

<br><br><br>
<h2>Samples images from our VisualAtom dataset</h2>

<center>
        <img src="./img/example_of_visual_atom.png" style="width: 100%;"/> 
</center>

<br><br><br>
<h2>Comparison: Fine-tuning on ImageNet, CIFAR, Cars, Other datasets</h2>

We found that vision transformers (ViT) can have higher pre-training effect if pre-trained on the dataset which has the rich variation of contour shapes among categories. 
We also found that our proposed VisualAtom outperforms all existing FDSL and some SL/SSL methods in classification tasks using real images (such as fine-tuning on ImageNet-1k).
<br><br><br>
<center>
    <p style="width: 100%;"> Comparison of pre-training methods. Best and second-best scores at ViT-Tiny are shown in underlined bold and bold, respectively. 
        At ViT-Base, best scores are shown in bold. </p>
    <img src="./img/comp_all_datasets.png" style="width: 100%;"/>
</center>
<br><br><br>
<center>
    <p style="width: 60%;"> Comparison with fine-tuning accuracy on ImageNet1k. Res. indicates the image resolution at fine-tuning.
        Best and second-best scores at each resolution and model scale are shown in underlined bold and bold, respectively. </p>
    <img src="./img/imagenet_comp.png" style="width: 60%;"/>
    <p style="width: 60%;"><small> * Since the JFT-300M, which is not available, cannot be compared with the same setting, we transcribe the accuracy with the different setting reported in the previous work. </small></p>
</center>

<br><br><br>
<h2>Systematically Investigate the Design Space of Contour-Oriented Synthetic Dataset</h2>


In order to find out the factors that improve the pre-training effect, we conducted an systematically investigation for image contour elements using VisualAtom. The following figures show the fine-tuning accuracies of varying the range of (A) frequency, (B) amplitude, and (C) quantization parameter of the contours which compose VisualAtom. Here, the frequency controls the outline shape, the amplitude controls the size of shape, and the quantization parameter controls the smoothness.
According to this investigation, we found that variety of contour shapes in the pre-training dataset is a crucial factor for achieving a superior pre-training effect.
<br><br><br>
<p style="width: 100%;"> (left) : (A) Fine-tuning accuracy when varying the range of frequency parameters n1, n2. (middle) : (B) Fine-tuning accuracy when varying the range of number of orbits K. (right) : (C) Fine-tuning accuracy when varying the range of quantization parameter q. </p>
<center>
    <img src="./img/va_1.png" style="width: 32%;"/>
    &nbsp;&nbsp;
    <img src="./img/va_2.png" style="width: 32%;"/>
    &nbsp;&nbsp;
    <img src="./img/va_3.png" style="width: 32%;"/>
</center>
<br><br><br>

<!-- <br>
<h2>Citation (preprint)</h2>
@article{takashima2023visual,<br>
&emsp;author    = {Takashima, Sora and Hayamizu, Ryo and Inoue, Nakamasa and Kataoka, Hirokatsu and Yokota, Rio},<br>
&emsp;title    = {Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves},<br>
&emsp;journal    = {arXiv preprint arXiv:2303.01112},<br>
&emsp;year      = {2023},<br>
}<br><br> -->

<br>
<h2>Citation</h2>
@InProceedings{takashima2023visual,<br>
&emsp;author    = {Takashima, Sora and Hayamizu, Ryo and Inoue, Nakamasa and Kataoka, Hirokatsu and Yokota, Rio},<br>
&emsp;title     = {Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves},<br>
&emsp;booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
&emsp;month     = {June},<br>
&emsp;year      = {2023},<br>
&emsp;pages     = {18579-18588}<br>
}<br><br>

<a name="dataset"><h2>Dataset Download</h2></a>
<ul>
    <li>
        VisualATom-1k (1k categories x 1k instances; Total 1M images). <br>
        Please see <a href="https://github.com/masora1030/CVPR2023-FDSL-on-VisualAtom#visualatom-construction-readme">[GitHub]</a> for the rendering code. <br> 
        Please see <a href="https://zenodo.org/record/7945009">[zenodo]</a> for the raw dataset.
    </li>
</ul>

<h2>Acknowledgement</h2></a>
<ul>
    <li> This work is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO).</li>
    <li> Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.</li>
</ul>

<script type="text/javascript" src="./footer.js"></script>

</body></html>