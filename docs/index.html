<!DOCTYPE html>
<html>
<head>
	<title>Visual Atoms: Pre-training Vision Transformers with Sinusoidal Wave</title>
    <link rel="stylesheet" type="text/css" href="./pvg.css">
    <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
    <!--<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">-->
</head>

<body>
<script type="text/javascript" src="./header.js"></script>

<style>
a.myclass {
    color:#DE382D;
    text-decoration: underline
}
</style>

<style>
a.link {
    text-decoration: underline
}
</style>

<h1 align="center" style="font-size: 30pt;"><b>Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves</b></h1><br/>
<center>
    <font color="#c7254e" font size="4"><b>IEEE/CVF International Conference on Computer Vision and Pattern Recognition<br>(CVPR 2023)</b></font><br><br>
    <a href="https://masora1030.github.io/soraemonpockt/" class="">Sora Takashima</a><sup>1,2</sup> &emsp;
    <a href="" class="">Ryo Hayamizu</a><sup>1</sup> &emsp;
    <a href="https://mmai.tech/" class="">Nakamasa Inoue</a><sup>1,2</sup> &emsp;
    <a href="http://hirokatsukataoka.net/" class="">Hirokatsu Kataoka</a><sup>1</sup> &emsp; 
    <a href="https://www.rio.gsic.titech.ac.jp/en/index.html" class="">Rio Yokota</a><sup>1,2</sup> &emsp;
<br>
    1: AIST &emsp; 2: TITech<br><br>
    
    <section class="delta">
        <div class="container">
            <a href="https://arxiv.org/pdf/2303.01112.pdf"><button class="btn btn-gray">Paper (preprint)</button></a>
            <a href="https://github.com/masora1030/CVPR2023-FDSL-on-VisualAtom"><button class="btn btn-gray">Code</button></a>
            <!-- <a href="#dataset"><button class="btn btn-gray">Dataset</button></a> -->
            <a href="#dataset"><button class="btn btn-gray">Dataset</button></a>
            <a href=""><button class="btn btn-gray">Oral (coming soon)</button></a>
            <a href=""><button class="btn btn-gray">Poster (coming soon)</button></a>
            <a href=""><button class="btn btn-gray">Supp (coming soon)</button></a> <br>
            <a href="https://hirokatsukataoka16.github.io/Pretraining-without-Natural-Images/"><button class="btn btn-gray">Related Work 1 (IJCV22)<br>FDSL Proposal</button></a>
            <a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/"><button class="btn btn-gray">Related Work 2 (AAAI22)<br>ViT Pre-training</button></a>
            <a href="https://hirokatsukataoka16.github.io/Replacing-Labeled-Real-Image-Datasets/"><button class="btn btn-gray">Related Work 3 (CVPR22)<br>ExFractalDB, RCDB</button></a>
        </div>
    </section>
    <br><br>
    <iframe width="800" height="450" src="https://www.youtube.com/embed/T_t9XXZYOYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <!--<img src="./img/teaser.png" style="width: 100%;"/>-->
</center>

<br>
<h2>Abstract</h2>
<p>
Formula-driven supervised learning (FDSL) has been shown to be an effective method for pre-training vision transformers, where ExFractalDB-21k was shown to exceed the pre-training effect of ImageNet-21k. These studies also indicate that contours mattered more than textures when pre-training vision transformers. However, the lack of a systematic investigation as to why these contour-oriented synthetic datasets can achieve the same accuracy as real datasets leaves much room for skepticism. <font color="#c7254e" font size="5"><b>In the present work, we develop a novel methodology based on circular harmonics for systematically investigating the design space of contour-oriented synthetic datasets. This allows us to efficiently search the optimal range of FDSL parameters and maximize the variety of synthetic images in the dataset, which we found to be a critical factor. When the resulting new dataset VisualAtom-21k is used for pre-training ViT-Base, the top-1 accuracy reached 83.7% when fine-tuning on ImageNet-1k. This is only 0.5% difference from the top-1 accuracy (84.2%) achieved by the JFT-300M pre-training, even though the scale of images is 1/14.</b></font> Unlike JFT-300M which is a static dataset, the quality of synthetic datasets will continue to improve, and the current work is a testament to this possibility. FDSL is also free of the common issues associated with real images, e.g. privacy/copyright issues, labeling costs/errors, and ethical biases.
</p>

<!-- <br><br><br>
<h2>Two hypotheses</h2>

To enhance the performance of FDSL, we test the following hypotheses 1 & 2:<br><br><br>

<font size="4">Hypothesis 1: Object Contours are what matter in FDSL datasets</font>
<ul>
  <li>In our preliminary study we found that attention was focused on the outer contours of the fractals</li>
  <li>We created a new dataset that consists only of contours â€“ Radial Contour DataBase (RCDB)</li>
  <li>Despite the lack of any texture, RCDB performed close to FractalDB and outperformed ImageNet-21k</li>
</ul>
<center>
        <img src="./img/hypothesis1.png" style="width: 60%;"/>
        <img src="./img/h1_exp.png" style="width: 39%;"/>
</center><br><br><br>

<font size="4">Hypothesis 2: Increased number of parameters in FDSL pre-training</font>
<ul>
  <li>We tested various synthetic datasets with varying complexity of images</li>
  <li>For RCDB, we changed the number of polygons, radius, line width, resizing factor, and Perlin noise</li>
  <li>Complex images increases the difficulty of the pre-training task and leads to better downstream performance</li>
</ul>
<center>
        <img src="./img/hypothesis2.png" style="width: 54%;"/> 
        <img src="./img/h2_exp.png" style="width: 45%;"/>
</center> -->

<!-- <br><br><br>
<h2>Samples images from our FDSL datasets</h2>

<center>
        <img src="./img/sample_images.png" style="width: 100%;"/> 
</center> -->

<!-- <br><br><br>
<h2>Comparison: ImageNet-1k, MS COCO, Other datasets</h2>

We have found that vision transformers (ViT) can be successfully pre-trained without real images, human- and self- supervision, and can exceed the accuracy of ImageNet-21k pre- training when fine-tuned on ImageNet-1k. We constructed a new dataset Radial Contour DataBase (RCDB) based on the as- sumption that contours are what matter for the pre-training of ViT. RCDB also exceeded the performance of ImageNet-21k pre- training, while consisting only of contours.
<center>
        <img src="./img/comparison_fdsl_sl.png" style="width: 100%;"/>
</center>
<br>

(Left) Comparison of ImageNet-1k fine-tuning. Accuracies obtained with ViT-Ti/B architectures are listed. 21k/50k indi- cates the number of classes in the pre-training phase. Best and second-best values for a given dataset are in underlined bold and bold, respectively. (Right) Comparison of object detection and instance segmen- tation. Several pre-trained models were validated on COCO dataset. Best values at each learning type are in bold.
<center>
        <img src="./img/comparison_imagenet1k_mscoco.png" style="width: 100%;"/>
</center>
<br>

Comparison of pre-training for SL/SSL methods. For SSL, (D) indicates DINO. Best values at each learning type are in bold.
<center>
        <img src="./img/comparison_ft.png" style="width: 100%;"/>
</center> -->

<!-- <br><br><br>
<h2>Failure modes: FractalDB and RCDB</h2>

We investigate the minimum number of points used in the rendering of fractals in FractalDB. The following figures shows the results and image examples in the point-rendered FractalDB. According to the performance rates in the figures, the pre-trained models acquire a good representation when the number of fractal points is 50k or higher, at which point the fractal images start to form a contour.
<center>
        <img src="./img/failuremodes_fractaldb.png" style="width: 80%;"/>
</center>
<br><br><br>

We verify RCDB images with and without broken object contours, as shown in the following figures. We deliberately draw 1k lines with the same color as the background. The lengths and positions of the lines are fully randomized. We adjust the thickness of the lines so that the object contours of RCDB are corrupted but the main frame does not disappear like in the figures. 
<center>
        <img src="./img/failuremodes_rcdb.png" style="width: 80%;"/>
</center> -->

<br>
<h2>Citation (preprint)</h2>
@article{takashima2023visual,<br>
&emsp;author    = {Takashima, Sora and Hayamizu, Ryo and Inoue, Nakamasa and Kataoka, Hirokatsu and Yokota, Rio},<br>
&emsp;title    = {Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves},<br>
&emsp;journal    = {arXiv preprint arXiv:2303.01112},<br>
&emsp;year      = {2023},<br>
}<br><br>

<!-- <br>
<h2>Citation</h2>
@InProceedings{takashima2023visual,<br>
&emsp;author    = {Takashima, Sora and Hayamizu, Ryo and Inoue, Nakamasa and Kataoka, Hirokatsu and Yokota, Rio},<br>
&emsp;title     = {Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves},<br>
&emsp;booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
&emsp;month     = {June},<br>
&emsp;year      = {2023},<br>
&emsp;pages     = {XXXXX-XXXXX}<br>
}<br><br> -->

<a name="dataset"><h2>Dataset Download</h2></a>
<ul>
    <li>
        VisualATom-1k (1k categories x 1k instances; Total 1M images). <br>
        Please see <a href="https://github.com/masora1030/CVPR2023-FDSL-on-VisualAtom#visualatom-construction-readme">[GitHub]</a> for the rendering code.
        <!--<a href="">[Dataset (TBD)]</a>-->
    </li>
</ul>
<br><br>

<h2>Acknowledgement</h2></a>
<ul>
    <li> This work is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO).</li>
    <li> Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.</li>
</ul>


<script type="text/javascript" src="./footer.js"></script>

</body></html>